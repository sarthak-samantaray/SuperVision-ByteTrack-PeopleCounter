{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1FMggEP0CmFwodvj0HBtXbL6AXBbZqRnS","authorship_tag":"ABX9TyPkwq9+kidStsHmFtI/3D89"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"1d24d29ad1254a06a380246d42a04ec4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a70b5e090b36439f8dc58676accad597","IPY_MODEL_687c6d8aace540a9bdb3b0bb354936ae","IPY_MODEL_7d822c4437d64e10962b049367e93ec8"],"layout":"IPY_MODEL_a7218b61b8b54f93bec024986b1eeec5"}},"a70b5e090b36439f8dc58676accad597":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84bcbc16ff144a6fbd1160b0107addac","placeholder":"​","style":"IPY_MODEL_b7379ced36324a2aba14f601a60a96ad","value":"100%"}},"687c6d8aace540a9bdb3b0bb354936ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a3ca7f5ed78402383d070b61f5f2ae2","max":632,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e636eb898888468893fe574c3cf56e53","value":632}},"7d822c4437d64e10962b049367e93ec8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b7f487e26eb484ebee1d95c71a9ecf7","placeholder":"​","style":"IPY_MODEL_8385287b63d7400c8b619c65f6172acb","value":" 632/632 [00:43&lt;00:00, 13.12it/s]"}},"a7218b61b8b54f93bec024986b1eeec5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84bcbc16ff144a6fbd1160b0107addac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7379ced36324a2aba14f601a60a96ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a3ca7f5ed78402383d070b61f5f2ae2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e636eb898888468893fe574c3cf56e53":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b7f487e26eb484ebee1d95c71a9ecf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8385287b63d7400c8b619c65f6172acb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4s7zCKGOpbL","executionInfo":{"status":"ok","timestamp":1682842580404,"user_tz":-330,"elapsed":30210,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"19f1ae8b-0619-434b-9363-ebbb88ad1ceb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os \n","os.chdir(\"/content/drive/MyDrive/AI projects/1. Car Counter\")\n","home = os.getcwd()\n","home"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4xM46EQ4UBkS","executionInfo":{"status":"ok","timestamp":1682843948756,"user_tz":-330,"elapsed":652,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"2d2f423e-1f91-4ba4-f3c3-e6938cbcb78d"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/AI projects/1. Car Counter'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["!pip install loguru\n","!pip install lap\n","!pip install ultralytics\n","\n","\n","!git clone https://github.com/ifzhang/ByteTrack.git\n","%cd {home}/ByteTrack\n","!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n","\n","!pip3 install -q -r requirements.txt\n","!python3 setup.py -q develop\n","!pip install -q cython_bbox\n","!pip install -q onemetric\n","\n","!pip install supervision==0.1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6SYXO_rUMsJ","executionInfo":{"status":"ok","timestamp":1682844065217,"user_tz":-330,"elapsed":9354,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"b45fecaf-b54d-4fe9-c195-a109a9b02025"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting supervision==0.1.0\n","  Downloading supervision-0.1.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from supervision==0.1.0) (1.22.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from supervision==0.1.0) (4.7.0.72)\n","Installing collected packages: supervision\n","Successfully installed supervision-0.1.0\n"]}]},{"cell_type":"code","source":["# Checking everything.\n","import ultralytics \n","print(ultralytics.checks())\n","\n","import supervision\n","print(f\"Supervision version = {supervision.__version__}\")\n"," \n","import yolox\n","print(f\"yolox verison = {yolox.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4zL9ZIAUP7R","executionInfo":{"status":"ok","timestamp":1682844089489,"user_tz":-330,"elapsed":17032,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"1ddda22d-6542-47b4-b059-686fea133674"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.90 🚀 Python-3.10.11 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete ✅ (2 CPUs, 12.7 GB RAM, 23.5/78.2 GB disk)\n"]},{"output_type":"stream","name":"stdout","text":["None\n","Supervision version = 0.1.0\n","yolox verison = 0.1.0\n"]}]},{"cell_type":"code","source":["from supervision.video.source import get_video_frames_generator\n","from supervision.draw.color import ColorPalette\n","from supervision.notebook.utils import show_frame_in_notebook\n","from supervision.tools.detections import Detections,BoxAnnotator\n","from supervision.video.sink import VideoSink # To save the video.\n","from supervision.video.dataclasses import VideoInfo\n","from supervision.tools.line_counter import LineCounter, LineCounterAnnotator\n","from supervision.geometry.dataclasses import Point\n","from tqdm.notebook import tqdm\n","import numpy as np\n","from ultralytics import YOLO"],"metadata":{"id":"TP8zfD64UrRi","executionInfo":{"status":"ok","timestamp":1682850739094,"user_tz":-330,"elapsed":673,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["from yolox.tracker.byte_tracker import BYTETracker, STrack\n","from onemetric.cv.utils.iou import box_iou_batch\n","from dataclasses import dataclass\n","\n","\n","@dataclass(frozen=True)\n","class BYTETrackerArgs:\n","    track_thresh: float = 0.25\n","    track_buffer: int = 30\n","    match_thresh: float = 0.8\n","    aspect_ratio_thresh: float = 3.0\n","    min_box_area: float = 1.0\n","    mot20: bool = False"],"metadata":{"id":"pX_xXgjReo1j","executionInfo":{"status":"ok","timestamp":1682850743369,"user_tz":-330,"elapsed":2,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["from typing import List\n","\n","import numpy as np\n","\n","\n","# converts Detections into format that can be consumed by match_detections_with_tracks function\n","def detections2boxes(detections: Detections) -> np.ndarray:\n","    # This will just horizontally stack the two values, looks like this [1,2,3,4,5] , 1 to 4 are the location, 5 is the conf.\n","    return np.hstack((\n","        detections.xyxy,\n","        # It makes confidence of each object in a seperate array. if conf = [1,2,3,4] then will change it into [1],[2],[3]...\n","        detections.confidence[:, np.newaxis]\n","    ))\n","\n","\n","# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n","# This will turn the tracks in to xmin,ymin,xmax,ymax\n","def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n","    return np.array([\n","        track.tlbr\n","        for track\n","        in tracks\n","    ], dtype=float)\n","\n","\n","\n","\n","\n","\n","\n","# This function takes in a set of detections and a list of tracks and matches the detections to the \n","# corresponding tracks based on their bounding box coordinates.\n","\n","# First, it checks if there are any detections or tracks. If there are none, it returns an empty array.\n","\n","# Next, it converts the tracks to bounding boxes using the tracks2boxes function and computes \n","# the intersection over union (IoU) between each track's bounding box and each detection's bounding box using the box_iou_batch function.\n","\n","# Then, it finds the index of the detection with the highest IoU for each track using np.argmax, and stores these indices in track2detection.\n","\n","# The function then initializes an empty list called tracker_ids with the same length as the number of detections. For each track, \n","# it checks if the highest IoU between the track and the detections is not zero. If it's not zero, it stores the track's ID in tracker_ids at \n","# the index corresponding to the detection with the highest IoU.\n","\n","# Finally, the function returns the list of tracker IDs for each detection.\n","\n","# matches our bounding boxes with predictions\n","def match_detections_with_tracks(\n","    detections: Detections, \n","    tracks: List[STrack]\n",") -> Detections:\n","    if not np.any(detections.xyxy) or len(tracks) == 0:\n","        return np.empty((0,))\n","\n","    tracks_boxes = tracks2boxes(tracks=tracks)\n","    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n","    track2detection = np.argmax(iou, axis=1)\n","    \n","    tracker_ids = [None] * len(detections)\n","    \n","    for tracker_index, detection_index in enumerate(track2detection):\n","        if iou[tracker_index, detection_index] != 0:\n","            tracker_ids[detection_index] = tracks[tracker_index].track_id\n","\n","    return tracker_ids"],"metadata":{"id":"RejVdnUyesLZ","executionInfo":{"status":"ok","timestamp":1682850743370,"user_tz":-330,"elapsed":2,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["# Settings \n","LINE_START = Point(0,550)\n","LINE_END = Point(1920,100)\n","\n","TARGET_VIDEO_PATH = f\"{home}/Videos/people-results101.mp4\""],"metadata":{"id":"JaSxZrXCetog","executionInfo":{"status":"ok","timestamp":1682850743370,"user_tz":-330,"elapsed":2,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["# The Flow\n","1. Video Source\n","2. Model\n","3. model.fuse()\n","4. class_id (the ones you want to be detected)\n","5. class_names_dict (the names of the classes)\n","6. initialize byte tracker\n","7. video_info(takes in source path)\n","8. create generator\n","9. create line counter instance\n","10. box_annotator instance\n","11. line annotator instance\n","\n","12. with videoSink(target_video,video_info) as sink:\n","13. loop over frames\n","14. result\n","15. detections\n","16. filtering out detections with unwanted classes\n","\n","17. tracking detections\n","18. extracting tracker id\n","19. filtering out detections without trackers\n","20. Labels\n","21. updating line counter\n","22. make bbox\n","23. make line\n","24. sink.write to save the video."],"metadata":{"id":"oUPSWb4_fNig"}},{"cell_type":"code","source":["# video source\n","SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/AI projects/1. Car Counter/Videos/people.mp4\"\n","\n","# Model\n","model = YOLO(f\"{home}/yolo_weights/yolov8x.pt\")\n","model.fuse()\n","\n","# CLASS ID\n","CLASS_ID = [0]\n","\n","# CLASS_NAMES\n","CLASS_NAMES_DICT = model.model.names\n","\n","# video info\n","video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n","\n","# Generator\n","generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n","\n","# Create LineCounter , BoxAnnotator , LineCounterAnnotator\n","line_counter = LineCounter(start = LINE_START , end = LINE_END)\n","box_annotator = BoxAnnotator(color = ColorPalette(), thickness = 1, text_thickness = 1, text_scale = 0.5)\n","line_annotator = LineCounterAnnotator(thickness=4, text_thickness=4, text_scale=2)\n","\n","# byte track instance\n","byte_tracker = BYTETracker(BYTETrackerArgs())\n","\n","# open target video file\n","with VideoSink(TARGET_VIDEO_PATH , video_info) as sink:\n","  # loop over frames\n","  for frame in tqdm(generator , total = video_info.total_frames):\n","    # model pred\n","    results = model(frame)\n","    detections = Detections(\n","        xyxy = results[0].boxes.xyxy.cpu().numpy(),\n","        confidence = results[0].boxes.conf.cpu().numpy(),\n","        class_id = results[0].boxes.cls.cpu().numpy().astype(int)\n","    )\n","\n","    # filtering\n","    mask = np.array([class_id in CLASS_ID for class_id in detections.class_id],dtype=bool)\n","    detections.filter(mask=mask,inplace=True)\n","\n","    # tracking\n","    tracks = byte_tracker.update(\n","        output_results = detections2boxes(detections=detections),\n","        img_info = frame.shape,\n","        img_size = frame.shape\n","    )\n","    # tracker id \n","    tracker_id = match_detections_with_tracks(detections=detections , tracks = tracks)\n","    detections.tracker_id = np.array(tracker_id)\n","\n","    # filtering\n","    mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id],dtype=bool)\n","    detections.filter(mask=mask,inplace=True)\n","\n","    # Labels \n","    labels = [\n","        f\"#{tracker_id} {CLASS_NAMES_DICT[class_id]} {confidence : 0.3f}\"\n","        for _,confidence,class_id,tracker_id in detections\n","    ]\n","\n","    # updating the line\n","    line_counter.update(detections = detections)\n","    \n","    #annotate and display\n","    frame = box_annotator.annotate(frame=frame , detections=detections,labels=labels)\n","    line_annotator.annotate(frame=frame , line_counter=line_counter)\n","    sink.write_frame(frame)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1d24d29ad1254a06a380246d42a04ec4","a70b5e090b36439f8dc58676accad597","687c6d8aace540a9bdb3b0bb354936ae","7d822c4437d64e10962b049367e93ec8","a7218b61b8b54f93bec024986b1eeec5","84bcbc16ff144a6fbd1160b0107addac","b7379ced36324a2aba14f601a60a96ad","1a3ca7f5ed78402383d070b61f5f2ae2","e636eb898888468893fe574c3cf56e53","9b7f487e26eb484ebee1d95c71a9ecf7","8385287b63d7400c8b619c65f6172acb"]},"id":"h0SR6Xe4foa5","executionInfo":{"status":"ok","timestamp":1682852098458,"user_tz":-330,"elapsed":45076,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"474dd3a2-b189-4343-e92c-1a2649c6d9fc"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/632 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d24d29ad1254a06a380246d42a04ec4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","0: 384x640 16 persons, 2 handbags, 64.5ms\n","Speed: 2.0ms preprocess, 64.5ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 handbags, 37.0ms\n","Speed: 2.0ms preprocess, 37.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 handbags, 35.1ms\n","Speed: 2.1ms preprocess, 35.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 handbags, 34.8ms\n","Speed: 2.0ms preprocess, 34.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 handbags, 31.1ms\n","Speed: 2.1ms preprocess, 31.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.1ms\n","Speed: 2.3ms preprocess, 31.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 31.2ms\n","Speed: 1.6ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 31.4ms\n","Speed: 2.0ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.5ms\n","Speed: 10.3ms preprocess, 30.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.4ms\n","Speed: 4.7ms preprocess, 30.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.1ms\n","Speed: 3.1ms preprocess, 30.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.7ms\n","Speed: 2.3ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.0ms\n","Speed: 2.6ms preprocess, 30.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.3ms\n","Speed: 2.3ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.4ms\n","Speed: 2.8ms preprocess, 30.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.6ms\n","Speed: 1.7ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.8ms\n","Speed: 1.9ms preprocess, 30.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 32.5ms\n","Speed: 1.8ms preprocess, 32.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 32.7ms\n","Speed: 1.7ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.2ms\n","Speed: 2.7ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.2ms\n","Speed: 1.9ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.5ms\n","Speed: 6.9ms preprocess, 30.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 31.1ms\n","Speed: 5.1ms preprocess, 31.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.1ms\n","Speed: 2.1ms preprocess, 31.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 persons, 31.9ms\n","Speed: 2.2ms preprocess, 31.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 persons, 1 handbag, 32.7ms\n","Speed: 3.9ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 persons, 1 handbag, 33.4ms\n","Speed: 1.9ms preprocess, 33.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 30.6ms\n","Speed: 2.3ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 persons, 1 handbag, 30.2ms\n","Speed: 2.4ms preprocess, 30.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 persons, 1 handbag, 32.2ms\n","Speed: 2.0ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 32.7ms\n","Speed: 1.8ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 31.0ms\n","Speed: 2.3ms preprocess, 31.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 31.7ms\n","Speed: 1.7ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 32.5ms\n","Speed: 2.0ms preprocess, 32.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 32.3ms\n","Speed: 2.2ms preprocess, 32.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 32.2ms\n","Speed: 1.7ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 32.2ms\n","Speed: 1.8ms preprocess, 32.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 30.3ms\n","Speed: 3.8ms preprocess, 30.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.7ms\n","Speed: 2.7ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.0ms\n","Speed: 2.6ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.2ms\n","Speed: 3.2ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.9ms\n","Speed: 2.0ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 31.2ms\n","Speed: 3.0ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.2ms\n","Speed: 1.9ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 32.7ms\n","Speed: 2.0ms preprocess, 32.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 handbags, 32.1ms\n","Speed: 2.0ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.7ms\n","Speed: 1.7ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.2ms\n","Speed: 2.3ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 32.2ms\n","Speed: 2.4ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 persons, 1 handbag, 30.3ms\n","Speed: 2.1ms preprocess, 30.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.0ms\n","Speed: 2.2ms preprocess, 31.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.2ms\n","Speed: 2.5ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.3ms\n","Speed: 3.1ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.3ms\n","Speed: 2.2ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 31.3ms\n","Speed: 2.4ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.6ms\n","Speed: 1.9ms preprocess, 31.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.3ms\n","Speed: 2.2ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 31.6ms\n","Speed: 2.2ms preprocess, 31.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.2ms\n","Speed: 1.6ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.5ms\n","Speed: 2.0ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.6ms\n","Speed: 2.1ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 30.4ms\n","Speed: 2.3ms preprocess, 30.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.3ms\n","Speed: 2.5ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.0ms\n","Speed: 2.2ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.5ms\n","Speed: 2.2ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 29.5ms\n","Speed: 4.6ms preprocess, 29.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.1ms\n","Speed: 1.7ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.5ms\n","Speed: 3.9ms preprocess, 30.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.6ms\n","Speed: 2.1ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.1ms\n","Speed: 1.9ms preprocess, 31.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.2ms\n","Speed: 2.1ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.2ms\n","Speed: 1.8ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.4ms\n","Speed: 2.1ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 30.5ms\n","Speed: 1.7ms preprocess, 30.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 30.6ms\n","Speed: 1.6ms preprocess, 30.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 1.9ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 38.5ms\n","Speed: 2.0ms preprocess, 38.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 32.0ms\n","Speed: 2.1ms preprocess, 32.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.7ms\n","Speed: 2.1ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.6ms\n","Speed: 2.2ms preprocess, 31.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.0ms\n","Speed: 2.6ms preprocess, 31.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 31.5ms\n","Speed: 1.7ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 handbag, 32.2ms\n","Speed: 1.9ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.8ms\n","Speed: 2.7ms preprocess, 30.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 35.1ms\n","Speed: 2.4ms preprocess, 35.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.8ms\n","Speed: 1.9ms preprocess, 30.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.6ms\n","Speed: 3.1ms preprocess, 30.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.6ms\n","Speed: 2.1ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.9ms\n","Speed: 2.2ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.9ms\n","Speed: 1.8ms preprocess, 31.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.5ms\n","Speed: 2.1ms preprocess, 31.5ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.3ms\n","Speed: 1.8ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.0ms\n","Speed: 2.5ms preprocess, 31.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 31.2ms\n","Speed: 2.1ms preprocess, 31.2ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 33.7ms\n","Speed: 4.7ms preprocess, 33.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 32.1ms\n","Speed: 2.2ms preprocess, 32.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 30.3ms\n","Speed: 2.0ms preprocess, 30.3ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.3ms\n","Speed: 3.2ms preprocess, 31.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 31.5ms\n","Speed: 3.4ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.1ms\n","Speed: 2.1ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 30.9ms\n","Speed: 2.2ms preprocess, 30.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.2ms\n","Speed: 2.3ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 31.8ms\n","Speed: 2.1ms preprocess, 31.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 handbag, 31.4ms\n","Speed: 2.3ms preprocess, 31.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 30.7ms\n","Speed: 2.1ms preprocess, 30.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 29.5ms\n","Speed: 3.9ms preprocess, 29.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.2ms\n","Speed: 4.3ms preprocess, 31.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 29.9ms\n","Speed: 4.5ms preprocess, 29.9ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 34.0ms\n","Speed: 2.0ms preprocess, 34.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 31.4ms\n","Speed: 2.1ms preprocess, 31.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 31.8ms\n","Speed: 2.9ms preprocess, 31.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.0ms\n","Speed: 2.1ms preprocess, 30.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 29.3ms\n","Speed: 3.2ms preprocess, 29.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.0ms\n","Speed: 2.6ms preprocess, 31.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 30.3ms\n","Speed: 1.9ms preprocess, 30.3ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.3ms\n","Speed: 2.1ms preprocess, 30.3ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 30.6ms\n","Speed: 4.7ms preprocess, 30.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 30.3ms\n","Speed: 2.0ms preprocess, 30.3ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 30.9ms\n","Speed: 2.2ms preprocess, 30.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 29.5ms\n","Speed: 2.0ms preprocess, 29.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 32.2ms\n","Speed: 2.0ms preprocess, 32.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 29.8ms\n","Speed: 1.8ms preprocess, 29.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.9ms\n","Speed: 2.5ms preprocess, 30.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 30.7ms\n","Speed: 2.2ms preprocess, 30.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 32.3ms\n","Speed: 1.9ms preprocess, 32.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 39.7ms\n","Speed: 2.1ms preprocess, 39.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 29.7ms\n","Speed: 2.3ms preprocess, 29.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.4ms\n","Speed: 2.2ms preprocess, 31.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.0ms\n","Speed: 2.3ms preprocess, 30.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 handbags, 29.8ms\n","Speed: 2.0ms preprocess, 29.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 30.0ms\n","Speed: 2.0ms preprocess, 30.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 31.2ms\n","Speed: 2.2ms preprocess, 31.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 29.7ms\n","Speed: 5.1ms preprocess, 29.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.4ms\n","Speed: 2.1ms preprocess, 31.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.1ms\n","Speed: 2.0ms preprocess, 30.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 32.3ms\n","Speed: 1.9ms preprocess, 32.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 32.7ms\n","Speed: 2.2ms preprocess, 32.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 32.6ms\n","Speed: 3.0ms preprocess, 32.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.2ms\n","Speed: 2.2ms preprocess, 30.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 29.3ms\n","Speed: 2.9ms preprocess, 29.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 29.9ms\n","Speed: 2.0ms preprocess, 29.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.4ms\n","Speed: 2.2ms preprocess, 30.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.4ms\n","Speed: 2.0ms preprocess, 30.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 29.4ms\n","Speed: 2.1ms preprocess, 29.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.7ms\n","Speed: 2.4ms preprocess, 30.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 29.7ms\n","Speed: 1.8ms preprocess, 29.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 35.5ms\n","Speed: 2.0ms preprocess, 35.5ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 29.7ms\n","Speed: 4.0ms preprocess, 29.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 30.3ms\n","Speed: 2.5ms preprocess, 30.3ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 29.6ms\n","Speed: 1.9ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 30.6ms\n","Speed: 2.0ms preprocess, 30.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 30.3ms\n","Speed: 2.2ms preprocess, 30.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.0ms\n","Speed: 2.2ms preprocess, 31.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 30.7ms\n","Speed: 4.2ms preprocess, 30.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.9ms\n","Speed: 2.2ms preprocess, 31.9ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 34.3ms\n","Speed: 2.1ms preprocess, 34.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 32.5ms\n","Speed: 2.1ms preprocess, 32.5ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 29.5ms\n","Speed: 4.7ms preprocess, 29.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 30.9ms\n","Speed: 3.8ms preprocess, 30.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.0ms\n","Speed: 2.0ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 2 handbags, 31.3ms\n","Speed: 5.2ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 31.2ms\n","Speed: 2.9ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 2 handbags, 31.1ms\n","Speed: 2.7ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 1 handbag, 31.9ms\n","Speed: 2.1ms preprocess, 31.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 30.5ms\n","Speed: 2.1ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 30.5ms\n","Speed: 2.7ms preprocess, 30.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.1ms\n","Speed: 1.9ms preprocess, 31.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 2 handbags, 1 suitcase, 31.6ms\n","Speed: 2.1ms preprocess, 31.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 2 handbags, 1 suitcase, 31.1ms\n","Speed: 1.6ms preprocess, 31.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 32.5ms\n","Speed: 2.1ms preprocess, 32.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 30.8ms\n","Speed: 3.4ms preprocess, 30.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.0ms\n","Speed: 2.1ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 31.2ms\n","Speed: 2.2ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 2 handbags, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 2 handbags, 32.2ms\n","Speed: 2.2ms preprocess, 32.2ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 30.5ms\n","Speed: 2.2ms preprocess, 30.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 30.4ms\n","Speed: 2.2ms preprocess, 30.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.4ms\n","Speed: 2.2ms preprocess, 31.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 31.5ms\n","Speed: 2.3ms preprocess, 31.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 30.8ms\n","Speed: 2.3ms preprocess, 30.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 31.6ms\n","Speed: 2.1ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.6ms\n","Speed: 2.8ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 31.5ms\n","Speed: 2.1ms preprocess, 31.5ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 2 handbags, 31.1ms\n","Speed: 2.1ms preprocess, 31.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 32.0ms\n","Speed: 2.0ms preprocess, 32.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 bicycle, 1 handbag, 30.9ms\n","Speed: 2.1ms preprocess, 30.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 2 handbags, 30.9ms\n","Speed: 2.0ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 bicycle, 1 handbag, 31.6ms\n","Speed: 2.1ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 1 handbag, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 bicycle, 1 handbag, 30.2ms\n","Speed: 3.4ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 bicycle, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 1 train, 30.9ms\n","Speed: 2.2ms preprocess, 30.9ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 2.3ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 30.7ms\n","Speed: 2.2ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 32.8ms\n","Speed: 2.0ms preprocess, 32.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 30.4ms\n","Speed: 2.9ms preprocess, 30.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 1 handbag, 31.5ms\n","Speed: 2.0ms preprocess, 31.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 1.7ms preprocess, 31.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 1 handbag, 31.6ms\n","Speed: 1.9ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.7ms\n","Speed: 2.3ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.6ms\n","Speed: 2.3ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 32.0ms\n","Speed: 3.5ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.9ms\n","Speed: 1.8ms preprocess, 31.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 train, 32.3ms\n","Speed: 2.2ms preprocess, 32.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 train, 30.7ms\n","Speed: 1.8ms preprocess, 30.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 31.0ms\n","Speed: 2.4ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 31.4ms\n","Speed: 2.1ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 2 handbags, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 2 handbags, 35.3ms\n","Speed: 1.9ms preprocess, 35.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 2 handbags, 32.1ms\n","Speed: 2.5ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 2 handbags, 30.2ms\n","Speed: 2.6ms preprocess, 30.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 2 handbags, 31.9ms\n","Speed: 2.0ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.0ms\n","Speed: 2.6ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.0ms\n","Speed: 1.9ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 2 bicycles, 1 handbag, 1 suitcase, 31.5ms\n","Speed: 2.8ms preprocess, 31.5ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 1 suitcase, 30.1ms\n","Speed: 2.2ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.7ms\n","Speed: 2.5ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 2 handbags, 1 suitcase, 30.1ms\n","Speed: 3.4ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 2 handbags, 32.0ms\n","Speed: 2.1ms preprocess, 32.0ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 2 handbags, 32.0ms\n","Speed: 2.1ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.1ms\n","Speed: 2.1ms preprocess, 31.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.4ms\n","Speed: 2.0ms preprocess, 31.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.7ms\n","Speed: 1.9ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 30.9ms\n","Speed: 1.9ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 32.6ms\n","Speed: 1.9ms preprocess, 32.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 30.2ms\n","Speed: 2.4ms preprocess, 30.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 33.4ms\n","Speed: 1.9ms preprocess, 33.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.6ms\n","Speed: 2.2ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 33.7ms\n","Speed: 2.3ms preprocess, 33.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.6ms\n","Speed: 2.2ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.4ms\n","Speed: 2.1ms preprocess, 31.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 1 suitcase, 30.0ms\n","Speed: 2.3ms preprocess, 30.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 30.6ms\n","Speed: 2.0ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.2ms\n","Speed: 2.0ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.9ms\n","Speed: 2.4ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.6ms\n","Speed: 3.5ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 handbag, 31.2ms\n","Speed: 2.2ms preprocess, 31.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 32.4ms\n","Speed: 2.0ms preprocess, 32.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 30.3ms\n","Speed: 2.7ms preprocess, 30.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.0ms\n","Speed: 1.9ms preprocess, 31.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 handbag, 31.6ms\n","Speed: 2.1ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.4ms\n","Speed: 1.8ms preprocess, 31.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 2 bicycles, 1 handbag, 32.6ms\n","Speed: 2.3ms preprocess, 32.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 handbag, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.7ms\n","Speed: 2.1ms preprocess, 31.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.5ms\n","Speed: 2.1ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 2 bicycles, 1 handbag, 31.3ms\n","Speed: 1.6ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 2 bicycles, 1 handbag, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.9ms\n","Speed: 2.2ms preprocess, 31.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 30.8ms\n","Speed: 1.9ms preprocess, 30.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 32.9ms\n","Speed: 1.8ms preprocess, 32.9ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 32.2ms\n","Speed: 2.1ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 2 bicycles, 1 handbag, 32.6ms\n","Speed: 2.0ms preprocess, 32.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 bicycles, 1 handbag, 30.9ms\n","Speed: 1.8ms preprocess, 30.9ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 2 bicycles, 1 handbag, 30.7ms\n","Speed: 2.0ms preprocess, 30.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 33.0ms\n","Speed: 3.3ms preprocess, 33.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 2 bicycles, 1 handbag, 31.8ms\n","Speed: 1.8ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.6ms\n","Speed: 1.8ms preprocess, 31.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.3ms\n","Speed: 3.0ms preprocess, 31.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 handbag, 30.5ms\n","Speed: 2.9ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 30.9ms\n","Speed: 2.1ms preprocess, 30.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.6ms\n","Speed: 2.3ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.3ms\n","Speed: 2.5ms preprocess, 31.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 30.5ms\n","Speed: 4.7ms preprocess, 30.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 2 bicycles, 1 handbag, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.0ms\n","Speed: 2.3ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.1ms\n","Speed: 1.7ms preprocess, 31.1ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 handbag, 31.4ms\n","Speed: 3.4ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.2ms\n","Speed: 3.6ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 32.2ms\n","Speed: 2.2ms preprocess, 32.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 handbag, 31.4ms\n","Speed: 2.0ms preprocess, 31.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 handbag, 31.5ms\n","Speed: 2.0ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 handbag, 31.9ms\n","Speed: 1.9ms preprocess, 31.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 2 bicycles, 1 handbag, 31.0ms\n","Speed: 2.1ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 36.1ms\n","Speed: 1.9ms preprocess, 36.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 bicycles, 1 handbag, 36.2ms\n","Speed: 1.9ms preprocess, 36.2ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 23 persons, 2 bicycles, 1 handbag, 30.7ms\n","Speed: 1.7ms preprocess, 30.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 23 persons, 2 bicycles, 1 handbag, 1 suitcase, 32.4ms\n","Speed: 1.8ms preprocess, 32.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 23 persons, 2 bicycles, 1 handbag, 1 suitcase, 31.3ms\n","Speed: 1.7ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 2 bicycles, 1 handbag, 31.5ms\n","Speed: 2.0ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 32.2ms\n","Speed: 2.0ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.5ms\n","Speed: 2.1ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 30.6ms\n","Speed: 2.2ms preprocess, 30.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.0ms\n","Speed: 2.1ms preprocess, 31.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.7ms\n","Speed: 2.1ms preprocess, 31.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.8ms\n","Speed: 2.3ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 30.9ms\n","Speed: 7.0ms preprocess, 30.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 31.8ms\n","Speed: 2.0ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 1.9ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 30.5ms\n","Speed: 1.9ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 31.5ms\n","Speed: 2.2ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.9ms\n","Speed: 2.4ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 30.2ms\n","Speed: 2.1ms preprocess, 30.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 1 suitcase, 32.8ms\n","Speed: 2.1ms preprocess, 32.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 30.9ms\n","Speed: 2.2ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 31.9ms\n","Speed: 1.9ms preprocess, 31.9ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 32.3ms\n","Speed: 2.2ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 2.4ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 32.4ms\n","Speed: 2.6ms preprocess, 32.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 30.7ms\n","Speed: 2.0ms preprocess, 30.7ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 handbag, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 handbag, 30.3ms\n","Speed: 2.0ms preprocess, 30.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 handbag, 31.5ms\n","Speed: 2.1ms preprocess, 31.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 handbag, 30.1ms\n","Speed: 1.9ms preprocess, 30.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 33.6ms\n","Speed: 1.9ms preprocess, 33.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.3ms\n","Speed: 1.8ms preprocess, 31.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 bus, 1 handbag, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 bus, 1 handbag, 32.5ms\n","Speed: 1.8ms preprocess, 32.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 2 bicycles, 1 bus, 1 handbag, 1 suitcase, 30.2ms\n","Speed: 1.9ms preprocess, 30.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 2 bicycles, 1 bus, 1 handbag, 1 suitcase, 31.8ms\n","Speed: 2.6ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 2 bicycles, 1 bus, 1 handbag, 1 suitcase, 32.5ms\n","Speed: 1.9ms preprocess, 32.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 bus, 1 handbag, 33.4ms\n","Speed: 1.9ms preprocess, 33.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 bus, 1 handbag, 35.1ms\n","Speed: 2.0ms preprocess, 35.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 1 bus, 1 handbag, 32.8ms\n","Speed: 1.8ms preprocess, 32.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 1 bus, 1 handbag, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 30.8ms\n","Speed: 2.5ms preprocess, 30.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.9ms\n","Speed: 2.3ms preprocess, 31.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 bicycles, 31.8ms\n","Speed: 2.0ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 bus, 31.7ms\n","Speed: 1.8ms preprocess, 31.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 31.8ms\n","Speed: 3.8ms preprocess, 31.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 30.1ms\n","Speed: 2.1ms preprocess, 30.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 31.0ms\n","Speed: 2.2ms preprocess, 31.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 30.3ms\n","Speed: 2.0ms preprocess, 30.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 33.2ms\n","Speed: 1.9ms preprocess, 33.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 30.4ms\n","Speed: 2.0ms preprocess, 30.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 30.0ms\n","Speed: 2.3ms preprocess, 30.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 29.8ms\n","Speed: 1.9ms preprocess, 29.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.6ms\n","Speed: 3.1ms preprocess, 31.6ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.3ms\n","Speed: 1.9ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 31.0ms\n","Speed: 2.3ms preprocess, 31.0ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 29.6ms\n","Speed: 1.9ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 30.8ms\n","Speed: 2.2ms preprocess, 30.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 31.4ms\n","Speed: 2.0ms preprocess, 31.4ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 31.9ms\n","Speed: 2.1ms preprocess, 31.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 31.2ms\n","Speed: 1.9ms preprocess, 31.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 33.3ms\n","Speed: 2.0ms preprocess, 33.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 30.4ms\n","Speed: 2.0ms preprocess, 30.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 29.5ms\n","Speed: 1.9ms preprocess, 29.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 29.6ms\n","Speed: 2.1ms preprocess, 29.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 31.3ms\n","Speed: 3.8ms preprocess, 31.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 handbag, 31.1ms\n","Speed: 2.2ms preprocess, 31.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 31.2ms\n","Speed: 2.2ms preprocess, 31.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 29.6ms\n","Speed: 1.9ms preprocess, 29.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 31.7ms\n","Speed: 5.0ms preprocess, 31.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 30.3ms\n","Speed: 1.9ms preprocess, 30.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 1.8ms preprocess, 31.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 32.4ms\n","Speed: 1.9ms preprocess, 32.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 30.7ms\n","Speed: 1.9ms preprocess, 30.7ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 29.6ms\n","Speed: 2.0ms preprocess, 29.6ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 32.5ms\n","Speed: 2.0ms preprocess, 32.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 1 suitcase, 30.9ms\n","Speed: 2.7ms preprocess, 30.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.0ms\n","Speed: 2.1ms preprocess, 31.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 32.6ms\n","Speed: 2.1ms preprocess, 32.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 1 suitcase, 31.0ms\n","Speed: 2.1ms preprocess, 31.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 31.2ms\n","Speed: 2.1ms preprocess, 31.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 handbags, 30.3ms\n","Speed: 2.0ms preprocess, 30.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 2 handbags, 32.0ms\n","Speed: 2.0ms preprocess, 32.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 2 handbags, 31.9ms\n","Speed: 3.3ms preprocess, 31.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 handbags, 32.0ms\n","Speed: 2.2ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 handbags, 30.6ms\n","Speed: 6.0ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 1 suitcase, 32.5ms\n","Speed: 2.0ms preprocess, 32.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 1 suitcase, 31.1ms\n","Speed: 2.6ms preprocess, 31.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 1 suitcase, 32.1ms\n","Speed: 2.0ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 33.6ms\n","Speed: 2.0ms preprocess, 33.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 1 suitcase, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 handbag, 1 suitcase, 29.7ms\n","Speed: 1.6ms preprocess, 29.7ms inference, 5.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 handbag, 31.3ms\n","Speed: 2.3ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 handbag, 31.6ms\n","Speed: 2.4ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 handbag, 31.9ms\n","Speed: 2.1ms preprocess, 31.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 handbag, 31.8ms\n","Speed: 2.0ms preprocess, 31.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 bicycle, 1 handbag, 30.9ms\n","Speed: 6.3ms preprocess, 30.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 32.2ms\n","Speed: 1.9ms preprocess, 32.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 31.2ms\n","Speed: 2.2ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 31.2ms\n","Speed: 2.1ms preprocess, 31.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 32.3ms\n","Speed: 1.8ms preprocess, 32.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 bicycles, 32.4ms\n","Speed: 1.9ms preprocess, 32.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 31.9ms\n","Speed: 2.2ms preprocess, 31.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 33.0ms\n","Speed: 2.4ms preprocess, 33.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 2 bicycles, 33.0ms\n","Speed: 2.3ms preprocess, 33.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 31.3ms\n","Speed: 3.2ms preprocess, 31.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 2 bicycles, 1 handbag, 31.1ms\n","Speed: 2.1ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 22 persons, 1 bicycle, 1 handbag, 32.1ms\n","Speed: 1.7ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 30.6ms\n","Speed: 1.7ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bicycle, 1 handbag, 32.9ms\n","Speed: 2.1ms preprocess, 32.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 1 handbag, 32.2ms\n","Speed: 2.1ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bicycle, 1 handbag, 1 suitcase, 31.4ms\n","Speed: 1.7ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 31.7ms\n","Speed: 2.1ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 23 persons, 31.3ms\n","Speed: 2.2ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 24 persons, 30.7ms\n","Speed: 2.3ms preprocess, 30.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 23 persons, 1 bicycle, 32.6ms\n","Speed: 2.1ms preprocess, 32.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 30.7ms\n","Speed: 2.1ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 bicycles, 32.3ms\n","Speed: 5.9ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 bicycles, 1 handbag, 31.8ms\n","Speed: 2.1ms preprocess, 31.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 bicycles, 1 handbag, 31.8ms\n","Speed: 2.3ms preprocess, 31.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 bicycles, 1 handbag, 1 suitcase, 30.8ms\n","Speed: 2.0ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 1 suitcase, 32.0ms\n","Speed: 1.9ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 handbag, 2 suitcases, 31.7ms\n","Speed: 2.2ms preprocess, 31.7ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 handbag, 32.0ms\n","Speed: 2.0ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 31.5ms\n","Speed: 2.6ms preprocess, 31.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 32.0ms\n","Speed: 2.1ms preprocess, 32.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 31.2ms\n","Speed: 2.1ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 2 handbags, 30.9ms\n","Speed: 2.0ms preprocess, 30.9ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 31.9ms\n","Speed: 4.2ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 30.7ms\n","Speed: 2.1ms preprocess, 30.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 32.7ms\n","Speed: 1.9ms preprocess, 32.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.9ms\n","Speed: 2.0ms preprocess, 31.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 31.1ms\n","Speed: 4.4ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 handbag, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 31.8ms\n","Speed: 2.0ms preprocess, 31.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 30.9ms\n","Speed: 2.4ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 33.4ms\n","Speed: 1.9ms preprocess, 33.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 31.5ms\n","Speed: 2.1ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 2.0ms preprocess, 31.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 31.2ms\n","Speed: 2.0ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 32.0ms\n","Speed: 1.9ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 33.3ms\n","Speed: 3.2ms preprocess, 33.3ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.3ms\n","Speed: 1.8ms preprocess, 31.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 29.9ms\n","Speed: 1.8ms preprocess, 29.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 32.6ms\n","Speed: 2.1ms preprocess, 32.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 2 bicycles, 1 handbag, 30.7ms\n","Speed: 2.0ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 31.6ms\n","Speed: 1.9ms preprocess, 31.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bicycle, 1 handbag, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.5ms\n","Speed: 2.0ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bicycle, 1 handbag, 31.7ms\n","Speed: 1.9ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 32.7ms\n","Speed: 2.2ms preprocess, 32.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 32.0ms\n","Speed: 1.9ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 34.3ms\n","Speed: 2.0ms preprocess, 34.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 30.6ms\n","Speed: 2.0ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.4ms\n","Speed: 2.2ms preprocess, 31.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 31.7ms\n","Speed: 1.6ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 32.0ms\n","Speed: 1.9ms preprocess, 32.0ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 handbag, 31.7ms\n","Speed: 1.8ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 handbag, 1 suitcase, 30.2ms\n","Speed: 2.1ms preprocess, 30.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 1 handbag, 1 suitcase, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 1 handbag, 1 suitcase, 31.7ms\n","Speed: 1.9ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 bus, 31.8ms\n","Speed: 2.1ms preprocess, 31.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 bus, 1 handbag, 32.4ms\n","Speed: 1.7ms preprocess, 32.4ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 bus, 31.3ms\n","Speed: 2.5ms preprocess, 31.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 bus, 31.0ms\n","Speed: 2.4ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 23 persons, 1 bicycle, 1 bus, 31.8ms\n","Speed: 2.0ms preprocess, 31.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 23 persons, 1 bicycle, 1 handbag, 32.2ms\n","Speed: 2.0ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 handbag, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bus, 1 handbag, 31.6ms\n","Speed: 1.7ms preprocess, 31.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 1 handbag, 33.6ms\n","Speed: 2.0ms preprocess, 33.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 32.1ms\n","Speed: 3.2ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 31.6ms\n","Speed: 2.2ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.9ms\n","Speed: 1.9ms preprocess, 31.9ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 handbag, 32.2ms\n","Speed: 1.9ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 bus, 1 handbag, 33.0ms\n","Speed: 2.6ms preprocess, 33.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 bus, 1 handbag, 31.8ms\n","Speed: 2.1ms preprocess, 31.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 bus, 1 handbag, 31.4ms\n","Speed: 5.1ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 1 handbag, 32.6ms\n","Speed: 1.9ms preprocess, 32.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.3ms\n","Speed: 1.6ms preprocess, 31.3ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 32.0ms\n","Speed: 1.8ms preprocess, 32.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 1 handbag, 31.9ms\n","Speed: 1.9ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 1 handbag, 32.1ms\n","Speed: 2.0ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 1 handbag, 31.7ms\n","Speed: 2.1ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 33.2ms\n","Speed: 2.2ms preprocess, 33.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 32.3ms\n","Speed: 2.8ms preprocess, 32.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 32.1ms\n","Speed: 2.0ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 31.1ms\n","Speed: 1.6ms preprocess, 31.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 32.3ms\n","Speed: 2.1ms preprocess, 32.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 30.6ms\n","Speed: 2.4ms preprocess, 30.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 34.8ms\n","Speed: 1.9ms preprocess, 34.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 31.7ms\n","Speed: 2.2ms preprocess, 31.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 32.1ms\n","Speed: 2.9ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 persons, 1 bus, 1 handbag, 33.4ms\n","Speed: 1.8ms preprocess, 33.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 32.0ms\n","Speed: 1.7ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 31.6ms\n","Speed: 1.9ms preprocess, 31.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 31.9ms\n","Speed: 2.0ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 32.3ms\n","Speed: 2.1ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 30.9ms\n","Speed: 1.9ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 31.8ms\n","Speed: 2.0ms preprocess, 31.8ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 31.8ms\n","Speed: 2.1ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 31.6ms\n","Speed: 1.9ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.6ms\n","Speed: 1.9ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 33.1ms\n","Speed: 2.0ms preprocess, 33.1ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.5ms\n","Speed: 2.1ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 31.4ms\n","Speed: 2.0ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 31.6ms\n","Speed: 2.9ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 31.6ms\n","Speed: 2.8ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 32.2ms\n","Speed: 2.3ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 1 handbag, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 30.9ms\n","Speed: 2.3ms preprocess, 30.9ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 33.1ms\n","Speed: 1.9ms preprocess, 33.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 31.6ms\n","Speed: 1.9ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 32.1ms\n","Speed: 1.8ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 32.1ms\n","Speed: 1.9ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 32.3ms\n","Speed: 2.0ms preprocess, 32.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 30.7ms\n","Speed: 2.0ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 34.0ms\n","Speed: 2.1ms preprocess, 34.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 handbag, 31.7ms\n","Speed: 2.6ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.8ms\n","Speed: 2.1ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.6ms\n","Speed: 3.5ms preprocess, 31.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 31.4ms\n","Speed: 1.8ms preprocess, 31.4ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 1 handbag, 31.5ms\n","Speed: 2.2ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 1 handbag, 32.5ms\n","Speed: 2.1ms preprocess, 32.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 32.6ms\n","Speed: 2.7ms preprocess, 32.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 handbag, 31.2ms\n","Speed: 1.7ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 handbag, 31.7ms\n","Speed: 2.9ms preprocess, 31.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 32.1ms\n","Speed: 1.8ms preprocess, 32.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.7ms\n","Speed: 2.1ms preprocess, 31.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 1 handbag, 29.7ms\n","Speed: 1.8ms preprocess, 29.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 33.3ms\n","Speed: 2.6ms preprocess, 33.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 29.7ms\n","Speed: 4.5ms preprocess, 29.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 29.8ms\n","Speed: 2.0ms preprocess, 29.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 handbag, 32.6ms\n","Speed: 1.9ms preprocess, 32.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 29.7ms\n","Speed: 2.1ms preprocess, 29.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 31.4ms\n","Speed: 2.0ms preprocess, 31.4ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 handbag, 30.8ms\n","Speed: 2.6ms preprocess, 30.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 30.7ms\n","Speed: 2.1ms preprocess, 30.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 32.1ms\n","Speed: 2.0ms preprocess, 32.1ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 handbag, 31.1ms\n","Speed: 2.1ms preprocess, 31.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 handbag, 31.7ms\n","Speed: 1.9ms preprocess, 31.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 handbag, 31.5ms\n","Speed: 2.1ms preprocess, 31.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 handbag, 31.0ms\n","Speed: 1.8ms preprocess, 31.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 35.1ms\n","Speed: 1.9ms preprocess, 35.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 31.6ms\n","Speed: 2.3ms preprocess, 31.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 handbag, 31.0ms\n","Speed: 2.0ms preprocess, 31.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 1 handbag, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bus, 1 handbag, 31.2ms\n","Speed: 4.1ms preprocess, 31.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bus, 30.7ms\n","Speed: 1.9ms preprocess, 30.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bus, 32.1ms\n","Speed: 2.0ms preprocess, 32.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bus, 1 handbag, 32.6ms\n","Speed: 1.8ms preprocess, 32.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bicycle, 1 bus, 32.1ms\n","Speed: 2.6ms preprocess, 32.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bus, 33.0ms\n","Speed: 1.8ms preprocess, 33.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 1 bus, 32.6ms\n","Speed: 2.2ms preprocess, 32.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bus, 32.2ms\n","Speed: 3.2ms preprocess, 32.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bus, 34.0ms\n","Speed: 3.1ms preprocess, 34.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bus, 31.1ms\n","Speed: 2.0ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bus, 30.6ms\n","Speed: 2.0ms preprocess, 30.6ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 1 handbag, 31.5ms\n","Speed: 1.9ms preprocess, 31.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 1 handbag, 30.6ms\n","Speed: 2.0ms preprocess, 30.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 handbag, 32.7ms\n","Speed: 1.9ms preprocess, 32.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 persons, 1 bus, 1 handbag, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 30.8ms\n","Speed: 1.9ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 1 bus, 1 handbag, 33.2ms\n","Speed: 2.0ms preprocess, 33.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 bus, 31.4ms\n","Speed: 1.9ms preprocess, 31.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 31.6ms\n","Speed: 2.0ms preprocess, 31.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 31.6ms\n","Speed: 2.3ms preprocess, 31.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 30.4ms\n","Speed: 1.9ms preprocess, 30.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 29.9ms\n","Speed: 2.0ms preprocess, 29.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bus, 31.8ms\n","Speed: 1.9ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bus, 30.5ms\n","Speed: 2.1ms preprocess, 30.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 1 bus, 32.7ms\n","Speed: 1.9ms preprocess, 32.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 1 bus, 32.7ms\n","Speed: 2.1ms preprocess, 32.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 32.4ms\n","Speed: 2.1ms preprocess, 32.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 31.4ms\n","Speed: 4.2ms preprocess, 31.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 1 bus, 32.9ms\n","Speed: 2.1ms preprocess, 32.9ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 31.3ms\n","Speed: 2.0ms preprocess, 31.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bus, 32.9ms\n","Speed: 2.3ms preprocess, 32.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bus, 31.7ms\n","Speed: 2.0ms preprocess, 31.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 33.5ms\n","Speed: 2.0ms preprocess, 33.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 1 bus, 32.9ms\n","Speed: 2.0ms preprocess, 32.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 persons, 33.4ms\n","Speed: 2.0ms preprocess, 33.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 16 persons, 31.6ms\n","Speed: 3.8ms preprocess, 31.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 31.6ms\n","Speed: 1.8ms preprocess, 31.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 33.0ms\n","Speed: 2.0ms preprocess, 33.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 31.0ms\n","Speed: 2.1ms preprocess, 31.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 30.4ms\n","Speed: 2.1ms preprocess, 30.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 31.9ms\n","Speed: 2.0ms preprocess, 31.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 32.8ms\n","Speed: 2.0ms preprocess, 32.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 20 persons, 32.8ms\n","Speed: 2.1ms preprocess, 32.8ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 33.2ms\n","Speed: 2.1ms preprocess, 33.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 21 persons, 33.1ms\n","Speed: 3.5ms preprocess, 33.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 33.0ms\n","Speed: 2.0ms preprocess, 33.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 33.0ms\n","Speed: 1.9ms preprocess, 33.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 31.9ms\n","Speed: 4.3ms preprocess, 31.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 1 bicycle, 30.8ms\n","Speed: 2.2ms preprocess, 30.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 31.5ms\n","Speed: 2.5ms preprocess, 31.5ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 19 persons, 32.0ms\n","Speed: 1.9ms preprocess, 32.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 18 persons, 31.9ms\n","Speed: 1.9ms preprocess, 31.9ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 17 persons, 1 bicycle, 32.0ms\n","Speed: 2.1ms preprocess, 32.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YeDPGPd4mz-5"},"execution_count":null,"outputs":[]}]}